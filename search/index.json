[{"content":"Introduction Open-source intelligence (OSINT) can be described as data that is publicly available and its analysis. As a user of the internet, regardless of your occupation and/or interests, you have (likely unwittingly) leveraged the powers of open-source intelligence, as data powers the digital world. For the cybersecurity professional, the ability to collect and analyze open-source intelligence is indispensable. The vast presence and inherent publicity of open-source intelligence can enable cybersecurity professionals to engage in in-depth analysis of threats and perform due diligence in other adjacent fields. This can also benefit an individual or organization by assisting them in the enumeration/identification of their surface area of exposure. Even in this digital world, there are troves of valuable information that often go unnoticed. That is the information present within data breaches/leaks. These breaches contain various data points that can prove to be incredibly valuable in many different ways.\nsearch.0t.rocks (previously known as search.illicit.services) was a search engine that enables users to efficiently search for specific records in compromised data. The original version/implementation hosted by MiyakoYakota leveraged the open-source search platform, Apache Solr, and served nearly 15 billion records towards the end of its operation. Some alternatives exist such as Dehashed and HaveIBeenPwned; however, these services often fall short. HaveIBeenPwned is free (API key is paid), but only presents whether or not a certain email is present in a data breach. It does not present any further information (apart from information relating to a data breach). Dehashed is a paid service and I have personally found the interface to be counterintuitive on occasion. IntelX\u0026rsquo;s pricing is exorbitant with the cheapest SMB \u0026amp; Enterprise tier being â‚¬2.500/$2,721.26 (as of the date/time of initially posting this). There are free tiers, but their capabilities are incredibly limited. This proves to be a barrier to most individuals. Search.0t.rocks was an entirely free service that presented this information in a consistent and minimal manner in spite of services like IntelX; however, it has recently shut down (once again). It is unlikely that it will be up and running again, so the developers have provided an open-source repository that enables individuals to build and serve it locally.\nSome barriers still exist, specifically relating to adding data to the self-hosted service as the user is personally required to clean and present data. There are also some (albeit only a few) issues with self-hosting search.0t.rocks. Some issues commonly arise during setup and configuration, and there is a lot of extra code that is unnecessary for the average user looking to self-host. Unfortunately, the code is a bit hard to understand which may discourage future maintenance, so I decided to rebuild the software from the ground-up, and I have titled this project Rapture. Rapture\u0026rsquo;s code base is incredibly lightweight, enabling the software to undergo further maintenance, development, and expansion with ease. The code, setup/configuration, and interface remain consistent with the core tenants of simplicity and usability exhibited by search.0t.rocks. It also leverages the Apache Solr search platform.\nOne might argue for the use of Glogg, Klogg, or a similar application that enables individuals to index and query large files; however, this is disadvantageous when handling multiple data breaches. You are required to index a file every time before querying it, and queries take a long time. With Rapture, which leverages Apache Solr, you only have to index the breach file a single time, queries are incredibly fast, you can host and access the service over the network, and perform boolean queries. These capabilities render Rapture\u0026rsquo;s user experience superior to the proposed alternatives. These differences are critical in time-sensitive investigations.\nThis article is a comprehensive guide to configuring Rapture locally, cleaning and converting data into a valid format, and importing the data into Rapture. Although this guide was designed for the configuration of Rapture, these steps are for the most part, cross-compatible with search.0t.rocks. These steps can also be performed in a headless manner, via remote connection to a machine.\nDisclaimers Although a disclaimer is already included within the Rapture repository, I feel it is important to reiterate considering the nature of this documentation:\nIt is the end user\u0026rsquo;s responsibility to obey all applicable local, state, and federal laws. Developers assume no liability and are not responsible for any misuse or damage caused by this program. This software comes AS IS with NO WARRANTY and NO GUARANTEES of any kind. By using Rapture, you agree to the previous statements.\nThe information below (within this disclaimer section) has been paraphrased by myself, but provided by Rob Volkert through OSINTCURIOUS (Direct | Archive). If you are looking to view the recommended guidelines in more detail, please view the full post via the link provided.\nAlthough this data is publicly available, it is incredibly important that you revisit any applicable legislation. There are also commonplace guidelines recognized by the community that your activities should remain consistent with:\nYou should not illegally profit from any compromised data You should not sell compromised data You should not solicit an individual to compromise data You should report any instances of illegal activity to the applicable law enforcement agencies Getting Started Prerequisites Git Docker (w/ Docker Compose) Java Runtime (8+) sudo apt install openjdk-19-jre-headless Installation The setup.sh script that is integrated within Rapture and automatically executed, relies on and/or creates a Solr collection by the name BigData in order to be cross-compatible with pre-existing search.0t.rocks installations/configurations.\nFetch repository from Github:\ngit clone https://github.com/ax-i-om/rapture.git\nChange directory\ncd rapture\nBuild via Docker Compose\ndocker compose build\nRun via Docker Compose\ndocker compose up -d\nIf you are following these instructions to setup search.0t.rocks, rather than Rapture, then you must execute the run.sh script that is included after step 4. If you are unable to successfully execute the run.sh script included with search.0t.rocks repository, please see the troubleshooting section below.\nAll necessary installation steps have been completed, and you should now have an instance of Rapture accessible on port 6175 (or otherwise configured port, may take a minute to initialize). If you chose to install search.0t.rocks instead, you should now have an instance accessible on port 3000 (or otherwise configured port). You can now move on to preparing your data for importation!\nData Conversion Now that you have successfully configured and launched an instance of Rapture and the technologies it depends on, it is time to prepare the data you wish to index. This part is where the difficulties reside, and the manners in which the datasets are prepared vary depending on the data points and structures presented. I would highly recommend that you familiarize yourself with a high-level programming language such as Python, as the ability to create and modify specialized scripts for preparing data is essential when handling large datasets. These examples will include hand-crafted samples that can be experimented on. All of the information/data presented in these examples were fabricated/manipulated/randomly generated; however, the formats of the datasets may share similar characteristics and fields present in many breaches. The data presented may also be formatted to be consistent with the \u0026ldquo;dirty\u0026rdquo; formats that may be present within real-world datasets. Fortunately in the case of many publicly-available datasets, they have already been cleaned to some extent which greatly reduces the amount of work for us.\nIn this section of the guide, I will provide various examples of how to use the Python language to format datasets that necessitate various levels of preparation. This section of the article is not designed to give you the answers on how to clean datasets, but to demonstrate the processes and workflows of cleaning such data. The ability to recognize patterns in datasets and employ solutions that leverage the patterns present is crucial. Be advised that there may not always be a perfect solution, and this process consists strongly of trial and error.\nYou may notice some patterns among the scripts presented in this section, as each script used is a derivative of a base script/algorithm that sequentially iterates through these large files. I will only explain concepts such as appending an ID key/value once. Further explanation would be redundant, and such implementations can easily be derived by viewing the provided solutions.\nThis section is subject to expansion in the future.\nExample One For the first example, we will be handling a sample dataset that has already been \u0026lsquo;cleaned,\u0026rsquo; but we will format it to be compatible with Rapture. The specific dataset provided below (ex1.csv) is in CSV format; however, other file/data formats are supported such as JSON and XML.\n1 2 3 4 5 6 7 8 9 10 email,password,firstName,lastName,userID,FBID rapture@demo.com,VK9RSuK0wSSXNc0gF8iYW1f6,axiom,estimate,4, rapture@demo.com,N7cblKU9ypU727lwiTr9espw,garble,vortex,5, rapture@demo.com,eP0u9cM0jAa2QeUVI3d88rYn,vertiable,slap,6, rapture@demo.com,QxtyRMAx3KniskzjGDg6tHdl,axiom,terrific,7, rapture@demo.com,cirSMQZp7Enh98KLb6r8JT1I,garble,eighty,8, rapture@demo.com,9J53HQEetTv5E2xCJKe4tdaP,veritable,tumble,9, rapture@demo.com,3sZ7NPb54Fk0Qy2LXlLejwCu,axiom,slipper,10, rapture@demo.com,OxoZGbn3v0tvBMyWA0Jds0Ea,garble,chord,11, rapture@demo.com,et8ggkgUeyQ3ge7ua2YNsOLd,veritable,baffle,12, Only the fields specified during the execution of setup.sh/run.sh are valid, and fields are case sensitive; therefore, firstName is a valid key/field, whereas firstname and FIRSTNAME are invalid. Likewise, the passwords field is valid; however, the singular iteration of the word (password) is invalid.\nIssues Even though this dataset appears to be well organized, it cannot be successfully posted to Rapture in its current state, as the file requires an id field. Although other values may be omitted, each record requires an id to be present. It is up to you how you format the id value; however, it is important that each and every occurrence of an id is unique. In the case of a duplicate id, data will be overwritten. If you fail to properly provide a record identifier, you will get the following error message when attempting to import your data:\nDocument is missing mandatory uniqueKey field: id\nWe can also observe that the email and password fields do not match the fields we specified while executing the setup.sh script; therefore, we must modify these field names to emails and passwords respectively.\nAlthough I might add such fields in the future, the setup.sh script did not specify a userID or FBID field, further rendering this dataset incompatible with Rapture in its current state.\nSolution One In a CSV file, the first row contains the headers. As we will be preparing a script that iterates through the datasets line by line, we will ensure the first action we take is to modify the headers accordingly depending on our goal.\nAs each record requires a unique identifier (id), we will prefix the headers with id,. Next, we will append an s to the end of the email and password headers so that they correspond with the fields we specified in setup.sh. Finally, we will remove the userID and FBID headers from this file, as I will be omitting these data points in this solution. If you wish to preserve these data points, please see Solution Two. If you decide to follow this solution and remove the userID and FBID headers, be aware that you must also remove the corresponding values for each and every corresponding record (even if they do not have a value), otherwise you will see an error as such:\nCSVLoader: input=null, line=1,expected 5 values but got 7\nSince the CSV headers only occupy the first row, we can simply set the first iteration of our script to handle this modification, and then every subsequent iteration to modify the records accordingly. For each record, we must prefix a unique record identifier. As previously mentioned, using duplicate record identifiers will lead to the unintentional overwriting of data. It is recommended to use a common scheme for specifying a record identifier. For the intents of this guide, I will use the following data to create a unique identifier:\nrecord: 0 - This is the iteration. The first iteration (record 0), refers to the CSV headers (first line). So when the record iterator is equal to 0, we will modify the headers. After every iteration (every line of the CSV file), the value of record will increment by 1. So when we reach the first row following the headers, the value of record will be equal to 1.\nbreachDate: 01JAN1337 - This variable corresponds with the date in which the data breach occurred. Since my data sample was fabricated, I am also using a fabricated value.\ncleanDate: 15MAR2024 - This variable corresponds with the date in which the data was cleaned/prepared to be imported into Rapture. This way, if you import multiple batches of the same breach on different dates, you will maintain a unique record identifier.\ntitle: R4PTUR3 - The title typically corresponds with the website/service that the breach data corresponds with. Again, since this sample data was fabricated, I am also using a fabricated value here.\nUsing this data in our script will generate a record identifier that appears like so: R4PTUR3-01JAN1337-15MAR2024-1, where the 1 value will increment by 1 for every iteration. So the 100th converted record will have the unique record identifier of: R4PTUR3-01JAN1337-15MAR2024-100. Since we are removing the userID and FBID fields/headers, we must also remove the corresponding data for each row. This is actually quite simple due to the comma delimiting leveraged by the CSV (comma-separated values) format. In the header, we can observe that the 4th instance of a comma (,) is used to delimit the userID value, and subsequently the FBID value. Therefore, we can simply locate the index of this fourth comma and then slice the string, only preserving the contents up to the index of the fourth comma. We can (and will) apply this process to both the headers and the records.\nAfter slicing the string to remove the undesired values, we prefix the string with the unique record identifier we generated. We then use the replace() function to locate the email and password strings in the header and simply replace them with emails and passwords respectively. We then append the record to a new file, and append a newline (\u0026quot;\\n\u0026quot;).\nAfter performing the steps presented above, our header should look like so:\nBEFORE: email,password,firstName,lastName,userID,FBID\nAFTER: id,emails,passwords,firstName,lastName\nAfter applying the modifications, the records should also be formatted like so:\nBEFORE: rapture@demo.com,VK9RSuK0wSSXNc0gF8iYW1f6,axiom,estimate,4,\nAFTER: R4PTUR3-01JAN1337-15MAR2024-1,rapture@demo.com,VK9RSuK0wSSXNc0gF8iYW1f6,axiom,estimate\nHere is what the full, converted sample should look like after performing these steps:\n1 2 3 4 5 6 7 8 9 10 id,emails,passwords,firstName,lastName R4PTUR3-01JAN1337-15MAR2024-1,rapture@demo.com,VK9RSuK0wSSXNc0gF8iYW1f6,axiom,estimate R4PTUR3-01JAN1337-15MAR2024-2,rapture@demo.com,N7cblKU9ypU727lwiTr9espw,garble,vortex R4PTUR3-01JAN1337-15MAR2024-3,rapture@demo.com,eP0u9cM0jAa2QeUVI3d88rYn,vertiable,slap R4PTUR3-01JAN1337-15MAR2024-4,rapture@demo.com,QxtyRMAx3KniskzjGDg6tHdl,axiom,terrific R4PTUR3-01JAN1337-15MAR2024-5,rapture@demo.com,cirSMQZp7Enh98KLb6r8JT1I,garble,eighty R4PTUR3-01JAN1337-15MAR2024-6,rapture@demo.com,9J53HQEetTv5E2xCJKe4tdaP,veritable,tumble R4PTUR3-01JAN1337-15MAR2024-7,rapture@demo.com,3sZ7NPb54Fk0Qy2LXlLejwCu,axiom,slipper R4PTUR3-01JAN1337-15MAR2024-8,rapture@demo.com,OxoZGbn3v0tvBMyWA0Jds0Ea,garble,chord R4PTUR3-01JAN1337-15MAR2024-9,rapture@demo.com,et8ggkgUeyQ3ge7ua2YNsOLd,veritable,baffle This data can be successfully imported into rapture and subsequently queried. Below, I provide the script used to achieve this result.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 from itertools import islice def fInstance(base: str, search: str, n: int) -\u0026gt; int: x = base.find(search) while x \u0026gt;= 0 and n \u0026gt; 1: x = base.find(search, x+len(search)) n -= 1 return x def main(): path = \u0026#39;./ex1.csv\u0026#39; saveAs = open(\u0026#39;ex1-notpreserved.csv\u0026#39;,\u0026#39;a+\u0026#39;) batch_size = 100 breachDate = \u0026#34;01JAN1337\u0026#34; cleanDate = \u0026#34;15MAR2024\u0026#34; title = \u0026#34;R4PTUR3\u0026#34; record = 0 with open(path, \u0026#39;r\u0026#39;) as file: while True: lines = list(islice(file, batch_size)) if not lines: break for line in lines: if record == 0: resLine = \u0026#34;id,\u0026#34;+line[:fInstance(line, \u0026#34;,\u0026#34;, 4)] + \u0026#34;\\n\u0026#34; resLine = resLine.replace(\u0026#34;email\u0026#34;, \u0026#34;emails\u0026#34;) resLine = resLine.replace(\u0026#34;password\u0026#34;, \u0026#34;passwords\u0026#34;) saveAs.write(resLine) else: recordIdentifier = title + \u0026#34;-\u0026#34; + breachDate + \u0026#34;-\u0026#34; + cleanDate + \u0026#34;-\u0026#34; + str(record) saveAs.write(recordIdentifier + \u0026#34;,\u0026#34;+line[:fInstance(line, \u0026#34;,\u0026#34;, 4)] + \u0026#34;\\n\u0026#34;) record += 1 if __name__ == \u0026#34;__main__\u0026#34;: main() Solution Two If you wish to preserve the userID and FBID values, you must add the corresponding fields to the Solr schema. I also rename the FBID field to facebookID to remain cohesive with the other field naming schemes. You must also modify the headers as mentioned above, by adding the id header and appending an s to the email and password headers. Since we are not required to discovered the nth instance of a comma in a line (since we are not removing any fields/values), we can omit the fInstance() function.\nHere is what the full, converted sample should look like after performing these steps:\n1 2 3 4 5 6 7 8 9 10 id,emails,passwords,firstName,lastName,userID,facebookID R4PTUR3-01JAN1337-15MAR2024-1,rapture@demo.com,VK9RSuK0wSSXNc0gF8iYW1f6,axiom,estimate,4, R4PTUR3-01JAN1337-15MAR2024-2,rapture@demo.com,N7cblKU9ypU727lwiTr9espw,garble,vortex,5, R4PTUR3-01JAN1337-15MAR2024-3,rapture@demo.com,eP0u9cM0jAa2QeUVI3d88rYn,vertiable,slap,6, R4PTUR3-01JAN1337-15MAR2024-4,rapture@demo.com,QxtyRMAx3KniskzjGDg6tHdl,axiom,terrific,7, R4PTUR3-01JAN1337-15MAR2024-5,rapture@demo.com,cirSMQZp7Enh98KLb6r8JT1I,garble,eighty,8, R4PTUR3-01JAN1337-15MAR2024-6,rapture@demo.com,9J53HQEetTv5E2xCJKe4tdaP,veritable,tumble,9, R4PTUR3-01JAN1337-15MAR2024-7,rapture@demo.com,3sZ7NPb54Fk0Qy2LXlLejwCu,axiom,slipper,10, R4PTUR3-01JAN1337-15MAR2024-8,rapture@demo.com,OxoZGbn3v0tvBMyWA0Jds0Ea,garble,chord,11, R4PTUR3-01JAN1337-15MAR2024-9,rapture@demo.com,et8ggkgUeyQ3ge7ua2YNsOLd,veritable,baffle,12, This data CAN NOT be successfully imported into rapture and subsequently queried. If you attempt to import this data, you will receive the following error message:\nERROR: [doc=R4PTUR3-01JAN1337-15MAR2024-1] unknown field 'userID'\nIn order to fix this, we must add the necessary fields to our Solr schema. We can easily achieve this using curl.\ncurl 'http://127.0.0.01:8983/solr/BigData/schema?wt=json' -X POST -H 'Accept: application/json' --data-raw '{\u0026quot;add-field\u0026quot;:{\u0026quot;stored\u0026quot;:\u0026quot;true\u0026quot;,\u0026quot;indexed\u0026quot;:\u0026quot;true\u0026quot;,\u0026quot;name\u0026quot;:\u0026quot;userID\u0026quot;,\u0026quot;type\u0026quot;:\u0026quot;string\u0026quot;}}'\ncurl 'http://127.0.0.01:8983/solr/BigData/schema?wt=json' -X POST -H 'Accept: application/json' --data-raw '{\u0026quot;add-field\u0026quot;:{\u0026quot;stored\u0026quot;:\u0026quot;true\u0026quot;,\u0026quot;indexed\u0026quot;:\u0026quot;true\u0026quot;,\u0026quot;name\u0026quot;:\u0026quot;facebookID\u0026quot;,\u0026quot;type\u0026quot;:\u0026quot;string\u0026quot;}}'\nThese commands add the userID and facebookID fields, respectively. After executing these commands, we can successfully import and query the converted data. Below, I have provided the corresponding script used to convert the data while preserving the userID and facebookID fields.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 from itertools import islice def main(): path = \u0026#39;./ex1.csv\u0026#39; saveAs = open(\u0026#39;ex1-preserved.csv\u0026#39;,\u0026#39;a+\u0026#39;) batch_size = 100 breachDate = \u0026#34;01JAN1337\u0026#34; cleanDate = \u0026#34;15MAR2024\u0026#34; title = \u0026#34;R4PTUR3\u0026#34; record = 0 with open(path, \u0026#39;r\u0026#39;) as file: while True: lines = list(islice(file, batch_size)) if not lines: break for line in lines: if record == 0: resLine = \u0026#34;id,\u0026#34; + line resLine = resLine.replace(\u0026#34;email\u0026#34;, \u0026#34;emails\u0026#34;) resLine = resLine.replace(\u0026#34;password\u0026#34;, \u0026#34;passwords\u0026#34;) resLine = resLine.replace(\u0026#34;FBID\u0026#34;, \u0026#34;facebookID\u0026#34;) saveAs.write(resLine) else: recordIdentifier = title + \u0026#34;-\u0026#34; + breachDate + \u0026#34;-\u0026#34; + cleanDate + \u0026#34;-\u0026#34; + str(record) saveAs.write(recordIdentifier + \u0026#34;,\u0026#34;+line) record += 1 saveAs.close() if __name__ == \u0026#34;__main__\u0026#34;: main() Example Two In the second example, we will be handling a real-world dataset, specifically consisting of .txt files. The records that are contained within this dataset titled 2_2.txt which primarily consists of data compiled/aggregated from malware/stealer logs. The data presented here has been manipulated, and any sensitive credentials or personally identifiable information has been replaced.\n1 2 3 4 5 6 7 8 9 https://osu.ppy.sh/home/download:axiom:VK9RSuK0wSSXNc0gF8iYW1f6 https://www.facebook.com/:rapture@demo.com:N7cblKU9ypU727lwiTr9espw https://auth.riotgames.com/login:rapture@demo.com:eP0u9cM0jAa2QeUVI3d88rYn https://accounts.google.com/signin/v2/challenge/pwd:rapture@demo.com:QxtyRMAx3KniskzjGDg6tHdl https://discord.com/channels/881160035171001562/359675218011979118:rapture@demo.com:cirSMQZp7Enh98KLb6r8JT1I https://www.facebook.com/:rapture@demo.com:9J53HQEetTv5E2xCJKe4tdaP https://login.live.com/login.srf:rapture@demo.com:3sZ7NPb54Fk0Qy2LXlLejwCu https://www.twitch.tv/directory/game/VALORANT:axiom:OxoZGbn3v0tvBMyWA0Jds0Ea https://login.live.com/login.srf:rapture@demo.com:et8ggkgUeyQ3ge7ua2YNsOLd Issues The example dataset provided above seems fairly straightforward at first glance. It is a .txt file where the records (which appear to be domain:username/email:password) are delimited by a colon (:). This presents the first issue, as there are colons present in the address bar. By using RegEx, this is pretty easily handled; however, not all of the URLs share the same format. There are various protocols indicated in the address bar throughout the dataset including: http, https, file, android, and ftp. Some of the URLs/addresses completely omit the protocol such as login.live.com. Some of the addresses are present on a local area network, and designated by localhost or a private IP address, and may or may not specify a port number. Some addresses are a public IP address that may or may not specify a port number, and there are also addresses that only start with //. I spent hours working on a RegEx pattern that covered all these bases, and got pretty close; however, there was one exception that I just could not solve so I ended up scrapping it. I tried to fix it by leveraging various AI-powered tools, and tried to simply generate a pattern with AI. These AI-powered endeavors were unsuccessful. I may attempt to develop a suitable RegEx pattern again in the future; however, this example is based on a real-world dataset consisting of hundreds of files so I ended up changing my workflow. Instead of trying to handle all these different edge cases, I simply split up my workload based on the count of colons (:) present in each line. If there are more than three colons present in a line, I write it to rejected.txt for further review. Any lines with three or less colons present were handled/process. As I write this, I realize another option for handling the colon delimiting would be to parse the line from the end rather than from the beginning.\nNow that we have decided how to handle the addresses (for now), we can move on to processing the credentials present in each record. I also tried to use RegEx patterns to identify whether the second field contained an email address, or if it was a password. This was a failure, as there were many malformed email addresses. I had attempted to implement prompting here that would ask the user for input when the script encountered a string that contained an \u0026ldquo;@\u0026rdquo; character but was not matched by the Regular Expression. The prompt would ask the user to review the string, and decide whether it was an email, username, or if they didn\u0026rsquo;t know. The script would then write the credential according to the user input; however, this is incredibly tedious when handling millions of records, and may be even more innaccurate. For the application in question, this distinction is likely counterintuitive so I ended up passing the value to both the usernames and emails field.\nThere are also some instances of backslashes and double quotes in these fields, so I will handle these occurrences accordingly by using the replace() function.\nSolution Here I have provided a script that implements this workflow. This solution also skips blank lines entirely.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 import re from itertools import islice OKDOMAINEXP = r\u0026#39;(((http|https|ftp|file|android):\\/\\/)|(\\/\\/)).*?(?=:)\u0026#39; LOGINEXP = r\u0026#39;.*?(?=:)\u0026#39; PASSWORDEXP = r\u0026#39;(?\u0026lt;=:).*\u0026#39; def main(): path = \u0026#39;./2_2.txt\u0026#39; saveAs = open(\u0026#39;ready.json\u0026#39;,\u0026#39;w\u0026#39;) rejected = open(\u0026#39;rejected.txt\u0026#39;,\u0026#39;a+\u0026#39;) batch_size = 100 breachDate = \u0026#34;COLLECTION\u0026#34; cleanDate = \u0026#34;25MAR2024\u0026#34; title = \u0026#34;R4PTUR3\u0026#34; record = 0 with open(path, \u0026#39;r\u0026#39;) as file: saveAs.write(\u0026#39;[\u0026#39;) while True: lines = list(islice(file, batch_size)) if not lines: break for line in lines: if line.count(\u0026#34;:\u0026#34;) \u0026gt; 3: rejected.write(line) continue recordIdentifier = title + \u0026#34;-\u0026#34; + breachDate + \u0026#34;-\u0026#34; + cleanDate + \u0026#34;-\u0026#34; + str(record + 1) try: if record != 0: saveAs.write(\u0026#34;,\u0026#34;) domain = re.search(f\u0026#39;{OKDOMAINEXP}\u0026#39;, line).group(0) next = re.sub(re.escape(domain) + \u0026#34;:\u0026#34;, \u0026#39;\u0026#39;, line) login = re.search(f\u0026#39;{LOGINEXP}\u0026#39;, next).group(0) password = re.search(f\u0026#39;{PASSWORDEXP}\u0026#39;, next).group(0) saveAs.write(\u0026#39;\\n\\t{\\n\u0026#39;) saveAs.write(\u0026#39;\\t\\t\u0026#34;id\u0026#34;:\u0026#34;\u0026#39; + recordIdentifier + \u0026#39;\u0026#34;,\\n\u0026#39;) saveAs.write(\u0026#39;\\t\\t\u0026#34;domain\u0026#34;:\u0026#34;\u0026#39; + domain.replace(\u0026#39;\\\\\u0026#39;, \u0026#39;\\\\\\\\\u0026#39;).replace(\u0026#39;\u0026#34;\u0026#39;, \u0026#39;\\\\\u0026#34;\u0026#39;) + \u0026#39;\u0026#34;,\\n\u0026#39;) saveAs.write(\u0026#39;\\t\\t\u0026#34;emails\u0026#34;:\u0026#34;\u0026#39; + login.replace(\u0026#39;\\\\\u0026#39;, \u0026#39;\\\\\\\\\u0026#39;).replace(\u0026#39;\u0026#34;\u0026#39;, \u0026#39;\\\\\u0026#34;\u0026#39;) + \u0026#39;\u0026#34;,\\n\u0026#39;) saveAs.write(\u0026#39;\\t\\t\u0026#34;usernames\u0026#34;:\u0026#34;\u0026#39; + login.replace(\u0026#39;\\\\\u0026#39;, \u0026#39;\\\\\\\\\u0026#39;).replace(\u0026#39;\u0026#34;\u0026#39;, \u0026#39;\\\\\u0026#34;\u0026#39;) + \u0026#39;\u0026#34;,\\n\u0026#39;) saveAs.write(\u0026#39;\\t\\t\u0026#34;passwords\u0026#34;:\u0026#34;\u0026#39; + password.replace(\u0026#39;\\\\\u0026#39;, \u0026#39;\\\\\\\\\u0026#39;).replace(\u0026#39;\u0026#34;\u0026#39;, \u0026#39;\\\\\u0026#34;\u0026#39;) +\u0026#39;\u0026#34;\\n\u0026#39;) saveAs.write(\u0026#34;\\t}\u0026#34;) except: if len(line) \u0026gt; 1: rejected.write(f\u0026#39;{line}\u0026#39;) record += 1 saveAs.write(\u0026#34;\\n]\u0026#34;) saveAs.close() rejected.close() if __name__ == \u0026#34;__main__\u0026#34;: By passing the sample dataset presented above to this script, we can observe that no lines were rejected. The results depicted in ready.json can be observed here:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [ { \u0026#34;id\u0026#34;:\u0026#34;R4PTUR3-COLLECTION-25MAR2024-1\u0026#34;, \u0026#34;domain\u0026#34;:\u0026#34;https://osu.ppy.sh/home/download\u0026#34;, \u0026#34;emails\u0026#34;:\u0026#34;axiom\u0026#34;, \u0026#34;usernames\u0026#34;:\u0026#34;axiom\u0026#34;, \u0026#34;passwords\u0026#34;:\u0026#34;VK9RSuK0wSSXNc0gF8iYW1f6\u0026#34; }, ... { \u0026#34;id\u0026#34;:\u0026#34;R4PTUR3-COLLECTION-25MAR2024-9\u0026#34;, \u0026#34;domain\u0026#34;:\u0026#34;https://login.live.com/login.srf\u0026#34;, \u0026#34;emails\u0026#34;:\u0026#34;rapture@demo.com\u0026#34;, \u0026#34;usernames\u0026#34;:\u0026#34;rapture@demo.com\u0026#34;, \u0026#34;passwords\u0026#34;:\u0026#34;et8ggkgUeyQ3ge7ua2YNsOLd\u0026#34; } ] This file can be indexed without issue.\nData Importation The process of importing data to Solr is very straightforward, as long as you have properly prepared your data. In order to start, you must first download and extract the Solr binaries from https://solr.apache.org/downloads.html. In this article, I will be using version 9.5.0.\nEnsure that you have installed a suitable version of the Java runtime. If you have not installed a suitable version of the Java runtime, Solr will likely inform you of this and present what versions are acceptable.\nThe file types considered/accepted by the Solr POST tools are as follows:\n1 XML,JSON,JSONL,CSV,PDF,DOC,DOCX,PPT,PPTX,XLS,XLSX,ODT,ODP,ODS,OTT,OTP,OTS,RTF,HTM,HTML,TXT,LOG Although there are plenty of acceptable file types, we will primarily use either XML, JSON, or CSV. Any errors that arise during the use of these commands are most likely caused by malformed data. After the successful usage of these commands, you will be able to query the indexed documents via Rapture or search.0t.rocks.\nbin/post 1 ~/Desktop/solr-9.5.0/bin/post -c BigData -p 8983 -host 127.0.0.1 formattedData.json ~/Desktop/solr-9.5.0/bin/post - path to post binary\n-c BigData - specify collection\n-p 8983 - specify which port Solr is running on\n-host 127.0.0.1 - specify the host address running Solr\nformattedData.json - contents to post to Solr\nbin/solr Although the above command leveraging the Solr POST tool works, the bin/post script is deprecated in favor of the bin/solr POST command. The syntax of the command does vary; however, it is still incredibly straightforward. I have converted the command displayed above to instead use the bin/solr POST command.\n1 ~/Desktop/solr-9.5.0/bin/solr post -c BigData -url http://127.0.0.1:8983/solr/BigData/update?commit=true formattedData.json ~/Desktop/solr-9.5.0/bin/solr - path to Solr binary\npost - specify the POST command\n-c BigData - specify collection\n-url http://127.0.0.1:8983/solr/BigData/update?commit=true - specify the update URL for the Solr collection\nformattedData.json - contents to post to Solr\nTroubleshooting I have been contacted various times by individuals experiencing issues while trying to execute the run.sh setup script that is included with the search.0t.rocks repository. In commit 4c71edd titled Update run.sh, the curl commands switched from using a hard-coded host address and port number (localhost:8983)to using variables specified by the user: ($ip:$port) This layout prevented the variables from expanding, meaning curl was using $ip:$port as the URL, literally. This lead to the error: curl: (3) URL using bad/illegal format or missing URL for each curl statement. This issue is fixed by switching the single quotes (\u0026rsquo;) surrounding the curl\u0026rsquo;s positional URL argument to double quotes (\u0026quot;) so that the variables could successfully expand. I opened a pull request that fixes the issue (Pull Request #6), but it has not yet been merged. In the meantime, please use the run.sh file provided in the pull request or fix your existing script according to the information presented herein.\nSometimes errors might occur when using the Solr binaries if you move them from their original directory (wherever you extracted the contents of the downloaded .tgz file to).\nFuture plans Although Rapture is technically viable and functioning in its current state, it is by no means finished. There are various plans for Rapture that will significantly improve the user experience. In the case any significant changes are made, I will properly update this article/guide with the corresponding information. The current plans for Rapture and this article are as follows:\nAdd more examples for cleaning data Create a script that simplifies the process of manipulating the Solr schema Adding, modifying, and removing fields ASCII Folding Etc\u0026hellip; Add password extended search Add extended search for license plate/VIN by leveraging the VIN library in Bitcrook Add NOT queries Further optimization/performance enhancement AI Disclosure The banner/header image featured in this article was generated using OpenAI\u0026rsquo;s DALL-E 2 AI system.\n","date":"2024-03-25T00:00:00Z","image":"https://ax-i-om.github.io/p/rapture/cover_hu7b94f7d589ab845443d47baa64b6f53d_305610_120x120_fill_q75_box_smart1.jpg","permalink":"https://ax-i-om.github.io/p/rapture/","title":"Rapture"},{"content":"Introduction Recently, I found myself using a program I had previously created named Netflip which allows me to \u0026ldquo;scrape paste sites for sensitive information.\u0026rdquo; It works by essentially brute-forcing the random string of characters used to generate paste URLs (e.g. https://rentry.co/ab12b/raw). The bolded area that says ab12b is what Netflip attempts to brute force, and is relatively easy considering the very short length of five characters. It allows you to filter the results via plain text and RegEx (Regular Expression) so that you don\u0026rsquo;t have to manually filter out ridiculous amounts of useless information.\nThis tool also includes a preview window that displays the contents of valid links within the interface, meaning you can get a glimpse of everything that is being scraped. During this process, I noticed some things that were occurring quite often. Many of these pastes included links to files/drives hosted by file sharing/file storage platforms such as Mega, Google Drive, Yandex Disk, and others. Of course, my curiosity led to me looking to see exactly what I had stumbled across. A lot of the links were no longer valid for whatever reason. The first valid link I found contained a text file, which held a Base64 encoded message. Decoding the message redirected me to another Mega link that, fortunately, was also valid. I was excited to follow this path and see what awaited at the end, but unfortunately, it was just a brush file. I continued my search and found more valid links that were not so boring. Inside these drives were heaps of stolen content of all different types. This was a crude violation of the Mega Terms of Service, Copyright, and Piracy Statutes.\nMonetization In most cases, these groups that distribute the stolen content don\u0026rsquo;t put a direct paywall in front of it. They use services like Linkvertise and other ad links that generate revenue from clicks and advertisements. They also increase their revenue by requiring their members to refer others before they can access anything. They may sometimes even form partnerships with other groups that redistribute stolen content. Below are images of messages inside of a discord group that distributes stolen content, where they require individuals to refer others to access specific content. They also provide instructions on how to easily refer others and even an extra section where you can pay if you don\u0026rsquo;t feel like inviting people.\nAs aforementioned, they don\u0026rsquo;t only require referrals or a single paywall. They also put everything behind ad links like Linkvertise, which requires individuals to complete certain tasks like reading sponsored articles, viewing advertisements, or downloading applications. Linkvertise grants earnings equal to around 9.35 USD per 1,000 clicks, or .00935 USD per click. One of these groups that uses paste sites and chains Linkvertise links to increase earnings has a paste that serves as a sort of \u0026ldquo;featured index,\u0026rdquo; where there are 22 content links. The first content link redirects to a Linkvertise page, and after completion, we see that 2,231 individuals have viewed the resulting page. That is 2,231 clicks. This paste redirects the user to one more Linkvertise page, which points to another paste. This second paste has 2,765 views. That is a total of 4,996 clicks or around 46.71 USD. The second listing on their list of 22 links collected a total of around 3,427 clicks or 32.04 USD. The third only used one instance of Linkvertise, collecting around 13.94 USD. The fourth earned around 20.93 USD. One link in the content list ended up broken, but the remaining 21 functioned properly.\nThe total revenue from the remaining 21 links was 720.87 USD, purely from Linkvertise. This also only came from their \u0026ldquo;featured\u0026rdquo; content, meaning there is much more that they are distributing via services like Telegram and Discord. That is an average of 34.33 USD for each folder/drive of (stolen) content.\nApproach I realized how much of an issue this is, but also how I can use this to my advantage. These links to Mega drives/files aren\u0026rsquo;t naturally indexed by search engines, meaning that their discovery is essentially impossible unless someone directly shared this link with you or publicly uploaded it somewhere. This previously meant that there was no straightforward answer for discovering bad actors using Mega as a hub for the storage and distribution of stolen content. We also couldn\u0026rsquo;t guess the Mega drive\u0026rsquo;s ID and/or decryption key in an efficient manner, considering the combined length/complexity; however, now that individuals have started using paste sites as a medium for distribution, we don\u0026rsquo;t have to. All we have to do is guess the five-character string that is used as the identifier for the paste, and we can guess these really fast. This also means that any content discovered through this process was meant to be shared, meaning we are not directly violating anyone\u0026rsquo;s privacy.\nAfter realizing this, I quickly got to work on developing a more refined solution, designed specifically to scrape and extract links to these file-sharing/storage drives. I salvaged and modified some code for generating a random string of characters using two arguments: the length of the string, and what characters to choose from. The paste site that Tempest currently scrapes from is Rentry.co, which uses a five-character long string consisting of lower case characters (a-z), upper case characters (A-Z), and numbers 0-9. We use this section of the code to brute force the Rentry.co paste identifier string until we get a hit. We differentiate between hits and misses by using the response\u0026rsquo;s status code. If the status code is 200 (OK), then we have a hit. If it is anything else, then it is a miss and we continue.\nAfter we get a hit, the next step is to check whether or not the raw paste contains a Mega link. This can be achieved relatively easily by using RegEx. Here is the expression I created that can accurately identify any valid Mega file and/or folder.\n1 https://mega.nz/(folder|file)/([a-zA-Z0-9]{0,8})#([a-zA-Z0-9_-]{43}|[a-zA-Z0-9_-]{22}) This will NOT extract Mega links that do not include a decryption key in the URL. Mega links quickly differentiate between files and folders in the URL. Both files and folders have an ID of eight characters in length, consisting of lower case characters (a-z), upper case characters (A-Z), and the numbers 0-9. This is followed by a pound (#) symbol, which separates the ID from the decryption key. The decryption key can consist of lower case characters (a-z), upper case characters (A-Z), and the numbers 0-9; however, the decryption key also may include underscores ( _ ) and dashes/hyphens ( - ). Unlike the IDs, the decryption keys do differentiate in length based on whether or not it is a file or folder. Folders have a decryption key of 22 characters in length, whereas files have a decryption key of 43 characters in length. We will use the same expression to later extract any identified links from the paste.\nNow that we have extracted our Mega link, we must check whether or not it is still online. There is not much use in scraping offline Mega links in our scenario. The Mega API (Application Programming Interface) makes this extremely easy. Here is what the request URL looks like in Tempest:\n1 https://g.api.mega.co.nz/cs?id=5644474\u0026amp;n=ID In this URL, we replace ID with the eight-character identifier in our extracted Mega link. This is quite easy to do using RegEx, thanks to the contextual characters present in the URL. Passing the identifier to this API will mainly return one of two values. If it returns a value of -2, then our Mega link is online. If it returns any other value (typically -9), then our Mega link is invalid.\nNow all that is left to do is to output the Mega link, where we both write it to the console and write it to a specified text file.\nThe program is quick, but unfortunately, it is bottlenecked by rate limiting. I decided not to bypass this to avoid causing any heavy loads or interfering with Rentry operations in any manner but may implement proxy list support to Tempest in the future to circumvent this issue. Without bypassing the rate limits, I was able to scrape 350 online Mega links in around 30 minutes total.\nHere is a link to the repository containing the program created and used in this article: Tempest\nResults Now I had to evaluate every one of these links which took around four hours total. The text file containing the links had each link separated by a new line. My method of annotation was to write a descriptive tag next to each link based on its contents. Here is a key to the annotations I used:\nINFRINGING - identifies content that infringes copyright/piracy statutes whilst not being obscene (movies, tv shows, music, video games, etc\u0026hellip;).\nOBSCENE - identifies content that is infringing and obscene/indecent.\nREASONABLE - identifies content that is empty or requires further inspection beyond simply viewing the content metadata (downloading, streaming, previewing, etc\u0026hellip;), but we can reasonably assume that the contents are infringing, obscene, or objectionable based on the remaining metadata (title, folder names, remaining marks, etc\u0026hellip;).\nOBJECTIONABLE - identifies content that falls under Mega\u0026rsquo;s definition of objectionable material, and typically represents exploitative materials.\nOTHER/NONE - identifies content that falls outside of the aforementioned categories, and according to inspection, isn\u0026rsquo;t exploitative, obscene, or infringing in any manner.\nUnfortunately, I was unable to mark any of the links as OTHER/NONE. Out of the 350 links, every single one of them was infringing or we could reasonably assume they were infringing.\nBefore continuing, here are some relevant clauses in the Mega Terms of Service under the \u0026ldquo;What you can\u0026rsquo;t do\u0026rdquo; section that we will be referencing.\n15.3 - infringe anyone elseâ€™s intellectual property (including but not limited to copyright) or other rights in any data; 15.7 - use our service to: 15.7.1 - to store, use, download, upload, share, access, transmit, or otherwise make available, data in violation of any law in any country (including to breach copyright or other intellectual property rights held by us or anyone else); 15.7.4 - to store, use, download, upload, share, access, transmit, or otherwise make available, unsuitable, offensive, obscene, or discriminatory information of any kind; 89.71% (314/350) of the folders/drives contained content that was infringing. 10.29% (36/350) of the folders/drives contained enough content/metadata which we could reasonably assume was infringing in some manner. These folders were typically empty, contained empty folders, or contained a zip file.\nThere were many different methods I employed to evaluate the contents of the drives while avoiding exposure. The first and most obvious way was to look at the metadata of the drive and the folders/files that it contained.\nIn the image above, we can see the title of the drive and the name of the 0B folder (censored). A quick search on the internet shows us that these groups do distribute stolen content, and use paste sites to distribute said content. We can also reference the size of the folder (we don\u0026rsquo;t need to reasonably assume with this drive, but this is a good example nonetheless). Large files/folders typically indicate lots of photos and videos, as these files generally utilize more space than something like a configuration file. We can also reference the metadata of remaining files, which usually contain group information/markings. For example, a group that marks every single file with its name.\nGroups who leave their logo:\nSome groups leave images, .pdf files, and .txt files containing methods of contact.\nOut of the 314 folders/drives that were guaranteed to contain infringing content, only 1.91% (6/314) of them were not obscene. This means their content consisted of media like movies, TV shows, video games, music, and other common forms of entertainment/media.\n94.9% (298/314) of them contained content that was obscene/indecent but doesn\u0026rsquo;t necessarily fall under the definition of objectionable content. Mega\u0026rsquo;s definition of objectionable content is child exploitation material, violent extremism, bestiality, zoophilia, gore, malware, hacked/stolen data, passwords or other material as defined in section 3 of the Films, Videos, and Publications Classification Act of 1993 or other seriously harmful material. Although these materials aren\u0026rsquo;t necessarily objectionable according to the above definitions, they are still against all of the aforementioned Mega Terms of Service clauses (15.3, 15.7.1, 15.7.4). 3.18% (10/314) of the folders/drives contained materials that could be described as objectionable.\nAccording to the Objectionable material section in Mega\u0026rsquo;s Takedown guidance policy, Mega \u0026ldquo;may take down or disable access to such material, close the userâ€™s account and provide account details and other data to the appropriate authorities as it sees fit.\u0026rdquo; Unfortunately, they make it explicitly clear that they are not obligated to do so. They also don\u0026rsquo;t provide any simple methods of reporting said content like many other services do. Unfortunately, I am unable to report an extremely large majority of the content as it does not \u0026ldquo;infringe on the copyright of someone I am authorized to represent.\u0026rdquo; Fortunately, Mega does offer an email address abuse@mega.nz for anyone to report any objectionable/abusive content, which I promptly used (August 7th, 2023) to report the 10 drives/files that contained content that could be described as objectionable. In less than 45 minutes, they responded. They were requesting more information regarding the provided drives/files, specifically what I consider a Terms of Service violation and a description of each link. I described how each link falls under their definition of objectionable material, and what specifically defines it as objectionable. In less than 25 minutes, all of the drives/files were removed and the user\u0026rsquo;s accounts were closed for \u0026ldquo;gross violation of MEGA\u0026rsquo;s Terms of Service.\u0026rdquo; They have shown that given the proper information, they are diligent in removing objectionable/exploitative content. Considering the ease of this, I plan to properly annotate and submit the remaining 304 links that contain infringing content in an attempt to have them removed and the accounts of those responsible for distributing them, closed.\nSummary Although a bit rough-edged, Tempest has shown that it is capable of efficiently extracting exploitative materials being distributed by using paste sites as a medium of discovery. This means that we can interfere with the distribution of said content more effectively.\nOf course, this is only the tip of the iceberg. There is a much deeper issue occurring below the surface; however, I plan to broaden Tempest\u0026rsquo;s discovery capabilities to assist in this fight. Instead of only searching for valid Mega links, I will also implement searches for Gofile, Bunkr, Dood, Google Drive, Yandex Disk, Cyberdrop, and Sendvid links. Alongside searching for other file-sharing services, Tempest will also search through other paste services such as pasteall.org, paste.in, and paste.ee. Another interesting capability I plan to add is recursive discovery, meaning that if a paste contains another paste, ad-link, or shortened URL, Tempest will automatically search said links for files, drives, or other links. Of course, I will likely implement proxy list support to bypass any rate-limiting, as that appears to be the primary bottleneck.\nUpdate (Sep 12, 2023) Tempest has undergone some significant changes since this post was published. Tempest now supports six new cloud storage/file sharing platforms along side Mega, specifically:\nBunkr Cyberdrop Dood Gofile Google Drive SendVid Alongside the implementation of these new modules, Tempest now extracts more information about extracted links (Upload Date, Drive/File Size, File Count, Video Length, Thumbnail, etc\u0026hellip;) where applicable. This metadata coupled with Tempest\u0026rsquo;s new ability to output said results to JSON/CSV files enables researchers/professionals to efficiently handle large amounts of scraped data in order to engage in advanced analytics. Tempest is also currently undergoing some significant modularization and code refactoring in order to streamline maintenance and collaboration while simultaneously accomodating users with an intuitive interface/experience. The end-level of modularity Tempest aims to achieve will enable virtually anyone to add their own modules to Tempest, or tailor Tempest to their specific needs (foundational functions such as Extract(), Validate(), and any applicable metadata extraction functions will always be exported).\nAI Disclosure The banner/header image featured in this article and the Tempest logo/symbol was generated using OpenAI\u0026rsquo;s DALL-E 2 AI system.\n","date":"2023-08-08T00:00:00Z","image":"https://ax-i-om.github.io/p/tempest/cover_hu00fdb40ad53a68944f2ced8f57021506_123411_120x120_fill_q75_box_smart1.jpg","permalink":"https://ax-i-om.github.io/p/tempest/","title":"Tempest"},{"content":"Introduction If you are reading this then chances are, you have encrypted your device\u0026rsquo;s drive(s). If someone were to steal your device then they would be unable to access your data, but what if they stole it when you have already unencrypted the device? Chances are, you wont have enough time to hold down the power button and shut off your computer. This is where BusKill comes in\nWhat exactly is Buskill? According to the official website (Website | Archive), Buskill is a \u0026ldquo;dead man switch triggered when a magnetic breakaway is tripped.\u0026rdquo; When the USB device is disconnected, it triggers the computer to lock or shutdown. This allows us to protect our data\u0026rsquo;s confidentiality and/or integrity from being compromised.\nDemo Here is a demonstration of the improvised Buskill cable in action on my old laptop running pop!_OS. Your browser doesn't support HTML5 video. Here is a link to the video instead. DI-Why? The concept of this \u0026ldquo;dead man switch\u0026rdquo; sounds great, but I did not want to order this hardware from another party (or order it at all) or have to manually arm/disarm the device every time. Fortunately, I found an article made by Michael Altfield (Website | Archive), the founder of Buskill, that shows us how we can create our own Buskill device for cheap. Michael provides a lot of valuable information in this article, but I had to improvise.\nThe first main issue I encountered was a lack of availability for the mentioned magnetic breakaway USB adapters. The one showcased in the article was discontinued and I have had no luck finding any other USB Type A breakaway connectors except for one (Website | Archive), but it was $36.58 + VAT for a relatively obscure product shipping from across the world.\nSome might ask about USB-C instead as there is an abundance of USB-C magnetic breakaway cables/adapters online. Referencing a few Reddit posts/comments and some individual experiences (Post), the current implementations of USB-C magnetic adapters \u0026ldquo;are not compliant with the USB specifications\u0026rdquo; and may damage devices. One user reported having their notebook \u0026ldquo;burned\u0026rdquo; following an accidental detach. The general concern is that your device may be damaged due to static electricity. I would assume that this is why the USB-A magnetic breakaway adapters were discontinued. I have since reached out to Griffin Technology asking if this is why, I will update this post when I receive a response. I am not one to blindly believe everything I read on the internet, but I am also not one to read the USB-C specifications as they are in excess of 300 pages and I have discovered a viable solution that doesn\u0026rsquo;t rely on a magnetic connection.\nDo it Requirements/Materials Computer that supports udev rules (I am running a debian distribution) USB adapter/extension cord USB device (preferably with a hole for a keyring/string) Carabiner (or keyring/paracord) Hardware First we will configure the Buskill cable itself. Take your carabiner (or keyring/paracord) and attach it to the USB device (which should have a hole). Plug your USB device into the adapter/extension cord. Ensure that the connection isn\u0026rsquo;t too tight, preventing the device from coming out. Also make sure that the connection isn\u0026rsquo;t too loose, where any slight movement would result in a disconnection. Connect the cable to your computer. I managed to find all of the parts I need just laying around in some storage bins, here is what it looks like:\nWhat shouldn\u0026rsquo;t you do? For anyone who is attempting this at home, I would advise that you find a strong, durable USB device that already has a hole for a carabiner/keyring. It is unlikely that a makeshift implementation will prove to be reliable; however, I still tried. I took a strong piece of wire that I found and bent it to serve as a keyhole. I place this wire on top of the flat side of a USB drive and super glued it, then I wrapped it with electrical tape. This was not the best of ideas, but I thought why not. I tested it with some weight and the wire slipped out at around 20lbs. I\u0026rsquo;m sure someone could achieve something greater on this front, but I am not that someone. Here are some images of the failed endeavor:\nCreating the script Now that you have your DIY Buskill cable, it is time to create the script. The script we create is what will be executed when our USB device is disconnected. In this case, we will be creating a script that quickly shuts off the computer.\nNavigate to your home directory: cd /home/user Create a sh file: touch buskill.sh With your preferred text editor, open the .sh file you just created. nano buskill.sh In the .sh file, enter the following: 1 2 #!/bin/sh shutdown -h now Save the script and quit the editor: Ctrl + X, Yes, Return This .sh script will immediately shutdown the computer. You can replace line 2 and onwards with whatever you want to happen when the program is executed such as locking the screen, destroying the LUKS headers, or anything else you can think of. Make sure that you leave the shebang (line 1) in the script so that the correct interpreter is used.\nFinding the ID model Before we configure our udev rule, we need to find out what the ID_MODEL of our USB device is.\nInsert the USB device that will be used with the Buskill cable, or the entire cable with the USB device attached. Open your terminal Execute the following command: udevadm monitor --environment --udev Remove your USB device/Buskill cable. Press Ctrl + C to stop monitoring. Scroll until you find the entry that matches your USB device and note down the ID_MODEL value. Configuring the udev rule Now that we have our cable and our script set up, its time for the important part. We need to create and configure a udev rule that will execute our program upon the disconnection of our Buskill cable. Udev grants us the ability to dynamically manage device events. In our specific scenario, we will be creating a udev rule that executes the script we previously created when we disconnect our Buskill cable\nOpen your terminal and navigate to the rules.d directory: cd /etc/udev/rules.d Create a .rules file: touch 00-usb-ID_MODEL.rules (replace ID_MODEL) with the ID_MODEL you noted previously. Open the file you just created with your preferred text editor: sudo nano 00-usb-ID_MODEL.rules Paste the following into the .rules file: 1 ACTION==\u0026#34;remove\u0026#34;, ENV{ID_MODEL}==\u0026#34;SanDisk_3.2Gen1\u0026#34;, ENV{DISPLAY}=\u0026#34;:0\u0026#34;, RUN+=\u0026#34;/bin/sh /home/USER/buskill.sh\u0026#34; Replace SanDisk_3.2Gen1 with the ID_MODEL you noted previously. Replace any instance of USER with your user (so that it points to the script you created in your home directory). Save the file and quit the editor: Ctrl + X, Yes, Return What exactly is all this? Well, lets break it down:\nACTION==\u0026quot;remove\u0026quot;\nACTION is a key (name) outlined by the udev man page. It matches the name of an event action. We have set this to remove because we want to run a script upon the removal/disconnection of the USB device. If you reference the image from above where we are obtaining the ID_MODEL of our device, you can also see the ACTION key and it\u0026rsquo;s value.\nENV{ID_MODEL}==\u0026quot;SanDisk_3.2Gen1\u0026quot;\nENV{key} is a key that is used for matching against a device property value. There are multiple different device properties such as ID_VENDOR, ID_VENDOR_ENC, and others; however, we are only interested in the ID_MODEL key as it allows us to consistently identify the USB device associated with our Buskill cable. We are using the == operator to compare between the ID_MODEL of the USB device that prompted the remove action with our Buskill cable\u0026rsquo;s ID_MODEL. In my case, the ID_MODEL is SanDisk_3.2Gen1. This allows us to specify what USB drive must be removed for our script to run. This way, we are still able to remove other USB devices without any interference. If you elect to remove this section, any USB device that is removed from your computer will cause the script to run.\nENV{DISPLAY}=\u0026quot;:0\u0026quot;\nAgain, we are taking advantage of ENV{key}. In this instance, however, we are not doing any comparisons. This time, we are using the = operator in order to set the value of DISPLAY to :0. We are using this to instruct our program to run on the default host of our machine.\nRUN+=\u0026quot;/bin/sh /home/msi/buskill.sh\u0026quot;\nThis is where we specify what interpreter to use and what program will be executed. Similar to how we configured the shebang in our buskill.sh script, we will specify the rule to execute our program with the Sh interpreter by prepending the program\u0026rsquo;s path with /bin/sh. We must ensure that we specify the absolute path for the script, as udev expects our program to reside in /lib/udev by default. In my case, the absolute path to the script is /home/user/buskill.sh.\nConclusion If everything has been properly configured, then your computer should quickly shutdown upon disconnection of the cable no matter the circumstances. In most circumstances, you would have the carabiner that is connected to you Buskill cable\u0026rsquo;s USB device attached to your belt-loop or some other part that is connected to you. In the case that an adversary attempts to separate you from your computer, the USB device will be disconnected from the Buskill cable, forcing the computer to shut down. Huge thanks to Michael Altfield for creating Buskill, and still showing us how to do it for free. If you are looking to purchase a Buskill cable, visit: https://www.buskill.in/store/. If you are looking for the official Buskill app (open-source), vist: https://github.com/BusKill/buskill-app. This post serves as documentation for the errors I encountered when configuring my own Buskill cable and the solutions I found. If you encounter any issues, feel free to contact me: addressaxiom@pm.me.\nAI Disclosure The banner/header image featured in this article was generated using OpenAI\u0026rsquo;s DALL-E 2 AI system.\n","date":"2023-06-18T00:00:00Z","image":"https://ax-i-om.github.io/p/bootleg-buskill/cover_hu364e670e80ccec120f78d613eb88d6fa_61625_120x120_fill_q75_box_smart1.jpg","permalink":"https://ax-i-om.github.io/p/bootleg-buskill/","title":"Bootleg Buskill"}]